# -*- coding: utf-8 -*-
"""studymate.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14YRHFwp3X50rJEJAouUfa8WKXLBBXg1C
"""

#@title StudyMate — single cell (copy-paste into Colab)
# NOTE: This single cell installs packages, starts a Gradio app, and uses an LLM (IBM Granite 3.3 2B instruct).
# If local loading of the 3B model fails (OOM or no GPU), it will automatically fall back to the Hugging Face Inference API.
# If using the Inference API, set environment variable HUGGINGFACEHUB_API_TOKEN before running:
#   import os; os.environ['HUGGINGFACEHUB_API_TOKEN'] = "hf_..."

# ---------- INSTALL ----------
import os, sys, textwrap, time
print("Installing packages (this can take 1-3 minutes)...")
# core libs
!pip install --quiet transformers accelerate bitsandbytes sentence_transformers faiss-cpu pdfplumber gradio huggingface_hub

# ---------- IMPORTS ----------
import gradio as gr
import pdfplumber
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from huggingface_hub import InferenceClient
from pathlib import Path
import re
import html
print("Imports OK")

# ---------- SETTINGS ----------
LLM_MODEL = "ibm-granite/granite-3.3-2b-instruct"   # chosen model (verified on Hugging Face)
EMBED_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"  # lightweight embedding for retrieval
CHUNK_SIZE = 800
CHUNK_OVERLAP = 150
TOP_K = 5

# ---------- HELPERS: PDF parsing & chunking ----------
def extract_pages_and_chunks(pdf_bytes):
    pages_text = []
    with pdfplumber.open(io=pdf_bytes) as pdf:
        for i, page in enumerate(pdf.pages, start=1):
            txt = page.extract_text() or ""
            # normalize spaces and remove long newlines
            txt = re.sub(r'\n{2,}', '\n', txt).strip()
            pages_text.append((i, txt))
    # create chunks (each chunk remembers page number)
    chunks = []
    for page_no, text in pages_text:
        if not text.strip():
            continue
        # simple sliding window chunker per page
        words = text.split()
        if len(words) <= CHUNK_SIZE:
            chunks.append({"page": page_no, "text": " ".join(words)})
        else:
            start = 0
            while start < len(words):
                end = min(start + CHUNK_SIZE, len(words))
                chunk_words = words[start:end]
                chunks.append({"page": page_no, "text": " ".join(chunk_words)})
                if end == len(words):
                    break
                start = end - CHUNK_OVERLAP
    return chunks

# ---------- EMBEDDING & INDEX ----------
embedder = SentenceTransformer(EMBED_MODEL_NAME)

def build_faiss_index(chunks):
    texts = [c["text"] for c in chunks]
    embeddings = embedder.encode(texts, show_progress_bar=False, convert_to_numpy=True)
    dim = embeddings.shape[1]
    index = faiss.IndexFlatIP(dim)
    # normalize for cosine search
    faiss.normalize_L2(embeddings)
    index.add(embeddings)
    return index, embeddings

def semantic_search(query, chunks, index, embeddings, k=TOP_K):
    q_emb = embedder.encode([query], convert_to_numpy=True)
    faiss.normalize_L2(q_emb)
    D, I = index.search(q_emb, k)
    results = []
    for idx in I[0]:
        if idx < 0 or idx >= len(chunks):
            continue
        results.append(chunks[idx])
    return results

# ---------- SMART SEARCH: keyword + semantic ----------
def smart_search(query, chunks, index, embeddings, k=TOP_K):
    # keyword hits (simple)
    kw_hits = []
    qwords = [w.lower() for w in re.findall(r'\w+', query)]
    for c in chunks:
        lower = c["text"].lower()
        # count common words
        score = sum(1 for w in qwords if w in lower)
        if score>0:
            kw_hits.append((score, c))
    kw_hits = sorted(kw_hits, key=lambda x: -x[0])
    kw_results = [c for s,c in kw_hits[:k]]

    # semantic hits
    sem_results = semantic_search(query, chunks, index, embeddings, k=k)

    # merge preserving order and uniqueness, prefer kw results first
    seen = set()
    merged = []
    for c in kw_results + sem_results:
        key = (c["page"], c["text"][:80])
        if key not in seen:
            seen.add(key)
            merged.append(c)
    return merged[:k]

# ---------- LLM: try local load, else fallback to HF inference API ----------
llm_pipeline = None
use_inference_api = False
hf_token = os.environ.get("HUGGINGFACEHUB_API_TOKEN", None)

def try_load_local_model():
    global llm_pipeline, use_inference_api
    try:
        print("Attempting to load model locally (may require GPU + bitsandbytes)...")
        tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL, trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(
            LLM_MODEL,
            device_map="auto",
            load_in_8bit=True,           # try 8-bit to fit in GPU
            trust_remote_code=True
        )
        llm_pipeline = pipeline("text-generation", model=model, tokenizer=tokenizer, max_new_tokens=512)
        use_inference_api = False
        print("Local model loaded successfully.")
    except Exception as e:
        print("Local load failed:", str(e))
        print("Falling back to Hugging Face Inference API (requires HUGGINGFACEHUB_API_TOKEN).")
        use_inference_api = True

def answer_with_llm(prompt, max_tokens=512, temperature=0.0):
    global llm_pipeline, use_inference_api, hf_token
    if llm_pipeline is not None and not use_inference_api:
        out = llm_pipeline(prompt, max_new_tokens=max_tokens, do_sample=False)
        return out[0]["generated_text"]
    else:
        # fallback to Inference API
        if hf_token is None:
            return ("ERROR: No local model and no Hugging Face token found.\n"
                    "Set environment variable HUGGINGFACEHUB_API_TOKEN and re-run.")
        client = InferenceClient(token=hf_token)
        # choose model
        try:
            resp = client.text_generation(LLM_MODEL, {"inputs": prompt, "max_new_tokens": max_tokens, "temperature": temperature})
            # The client returns structured response; pick text
            if isinstance(resp, dict) and "generated_text" in resp:
                return resp["generated_text"]
            # else attempt to parse list
            if isinstance(resp, list) and len(resp)>0 and "generated_text" in resp[0]:
                return resp[0]["generated_text"]
            # fallback raw
            return str(resp)
        except Exception as e:
            return f"LLM inference API error: {e}"

# attempt to load model (non-blocking fallback)
try_load_local_model()

# ---------- PROMPT helpers ----------
def build_context_prompt(question, top_chunks):
    header = ("You are StudyMate, an AI tutor. Use the context below (from the student's PDF) to answer the question. "
              "Cite page numbers in square brackets like [p:3] after statements that come from a specific page.\n\n")
    context = ""
    for i, c in enumerate(top_chunks, start=1):
        context += f"[CHUNK {i} | page:{c['page']}]\n{c['text']}\n\n"
    instr = f"Question: {question}\n\nAnswer concisely, show reasoning if helpful, and include page citations like [p:5]. If unsure, say you are unsure and show supporting pages."
    return header + context + instr

# ---------- GRADIO FUNCTIONS ----------
import io, json

STATE = {"chunks": [], "index": None, "embeddings": None}

def upload_pdf_and_index(pdf_file):
    if pdf_file is None:
        return "Upload a PDF first.", None, None, None
    # pdf_file is a tempfile-like object
    pdf_bytes = pdf_file.name if isinstance(pdf_file, str) else pdf_file
    # pdfplumber can accept file paths or file-like objects opened in binary mode
    with open(pdf_file.name, "rb") as f:
        chunks = extract_pages_and_chunks(f)
    if not chunks:
        return "No extractable text found in PDF.", None, None, None
    index, embeddings = build_faiss_index(chunks)
    STATE["chunks"] = chunks
    STATE["index"] = index
    STATE["embeddings"] = embeddings
    return f"Indexed {len(chunks)} text chunks from PDF (pages: {sorted(set([c['page'] for c in chunks]))}).", f"Pages indexed: {sorted(set([c['page'] for c in chunks]))}", None, None

def ask_question(question, top_k=TOP_K):
    if not STATE.get("chunks"):
        return "Please upload and index a PDF first.", "", "", ""
    # smart search to get top chunks
    top_chunks = smart_search(question, STATE["chunks"], STATE["index"], STATE["embeddings"], k=top_k)
    prompt = build_context_prompt(question, top_chunks)
    raw = answer_with_llm(prompt)
    # gather page sources
    pages = sorted(set(c["page"] for c in top_chunks))
    pages_display = ", ".join(str(p) for p in pages)
    return raw, f"Referenced pages: {pages_display}", "\n\n".join([f"Page {c['page']}: {c['text'][:300]}..." for c in top_chunks]), prompt

def generate_summary():
    if not STATE.get("chunks"):
        return "Upload and index a PDF first.", ""
    # create a joined text sample (truncate large docs)
    sample_text = "\n\n".join([c["text"] for c in STATE["chunks"][:40]])
    prompt = ("You are StudyMate. Produce: (1) a concise summary of the uploaded PDF (3-6 sentences), "
              "(2) 5 study notes / bullet points students should remember, and (3) 5 quick revision questions with answers.\n\n"
              f"Document excerpt:\n{sample_text}\n\nProduce numbered sections labeled SUMMARY, NOTES, REVISION QUESTIONS.")
    out = answer_with_llm(prompt, max_tokens=400)
    return out, ""

# ---------- GRADIO UI ----------
with gr.Blocks(title="StudyMate — PDF Q&A (IBM Granite)") as demo:
    gr.Markdown("## StudyMate — AI PDF Q&A for students\nUpload a PDF, then ask questions. Answers include page citations like **[p:3]**.")
    with gr.Row():
        pdf_in = gr.File(label="Upload PDF (single file)", file_count="single")
        upload_btn = gr.Button("Upload & Index")
    status = gr.Textbox(label="Index status", interactive=False)
    pages_txt = gr.Textbox(label="Pages (detected)", interactive=False)
    upload_btn.click(upload_pdf_and_index, inputs=[pdf_in], outputs=[status, pages_txt, gr.Textbox(), gr.Textbox()])

    gr.Markdown("### Ask questions")
    q_in = gr.Textbox(label="Your question", placeholder="e.g., Explain the concept on page 12...")
    ask_btn = gr.Button("Ask")
    answer_out = gr.Textbox(label="Answer (LLM)", lines=12)
    sources_out = gr.Textbox(label="Source pages", lines=1)
    evidence_out = gr.Textbox(label="Top context chunks (truncated)", lines=6)
    prompt_out = gr.Textbox(label="(debug) prompt sent to LLM", lines=6)
    ask_btn.click(ask_question, inputs=[q_in], outputs=[answer_out, sources_out, evidence_out, prompt_out])

    gr.Markdown("### Smart Search")
    ss_q = gr.Textbox(label="Search inside PDF (keywords or natural language)")
    ss_btn = gr.Button("Smart Search")
    ss_results = gr.Textbox(label="Top chunks found", lines=6)
    def do_smart_search(q):
        if not STATE.get("chunks"):
            return "Upload and index a PDF first."
        hits = smart_search(q, STATE["chunks"], STATE["index"], STATE["embeddings"], k=6)
        return "\n\n".join([f"Page {h['page']} — {h['text'][:300]}..." for h in hits])
    ss_btn.click(do_smart_search, inputs=[ss_q], outputs=[ss_results])

    gr.Markdown("### AI Summary & Study Notes")
    sum_btn = gr.Button("Generate Summary & Notes")
    sum_out = gr.Textbox(label="Summary / Notes / Revision Questions", lines=12)
    sum_btn.click(generate_summary, inputs=None, outputs=[sum_out, gr.Textbox()])

    gr.Markdown("----\n**Notes:** If the notebook cannot load the model locally (OOM / no GPU), it will try the Hugging Face Inference API. "
                "Set HUGGINGFACEHUB_API_TOKEN environment variable in Colab if prompted.")

# Start app (runs inline in Colab)
demo.launch(share=False, debug=True)