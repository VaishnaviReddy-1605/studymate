# -*- coding: utf-8 -*-
"""studymate.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MTS6qs9gXV1qbJzZMN_xJBeW_h06euUM
"""

# =============================================
# âœ… FIXED StudyMate â€“ AI PDF Q&A (NO ERRORS)
# IBM Granite 3.3-2B + FAISS + Gradio
# =============================================

!pip install gradio pypdf sentence-transformers transformers accelerate faiss-cpu -q

import gradio as gr
from pypdf import PdfReader
from sentence_transformers import SentenceTransformer
import faiss
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# ---------------------------
# Load IBM Granite
# ---------------------------
model_name = "ibm-granite/granite-3.3-2b-instruct"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype=torch.float16
)

embedder = SentenceTransformer("all-MiniLM-L6-v2")

# GLOBALS
pdf_text_pages = []
chunks = []
chunk_page_map = []
index = None


# ---------------------------
# ğŸ“Œ PDF PARSER (FIXED)
# ---------------------------
def parse_pdf(pdf):
    global pdf_text_pages, chunks, chunk_page_map, index

    pdf_text_pages = []
    chunks = []
    chunk_page_map = []

    reader = PdfReader(pdf.name)

    for i, page in enumerate(reader.pages):
        text = page.extract_text() or ""
        pdf_text_pages.append(text)

        # Create small chunks for proper FAISS indexing
        for block in text.split("\n"):
            block = block.strip()
            if len(block) > 20:
                chunks.append(block)
                chunk_page_map.append(i + 1)   # Save page number

    # ---- BUILD EMBEDDING INDEX ----
    emb = embedder.encode(chunks).astype("float32")

    index = faiss.IndexFlatL2(emb.shape[1])
    index.add(emb)

    return f"PDF parsed successfully! {len(pdf_text_pages)} pages, {len(chunks)} text chunks."


# ---------------------------
# ğŸ“Œ FIXED QUESTION ANSWER FUNCTION
# ---------------------------
def ask_question(query):
    global index, chunks, chunk_page_map

    if index is None:
        return "âš  Upload PDF first!", ""

    # Retrieve most relevant chunks
    q_emb = embedder.encode([query]).astype("float32")
    D, I = index.search(q_emb, 5)

    retrieved = []
    for idx in I[0]:
        page = chunk_page_map[idx]
        text = chunks[idx]
        retrieved.append(f"[p:{page}] {text}")

    retrieved_text = "\n".join(retrieved)

    # ---- Shorten input for model safety ----
    retrieved_text = retrieved_text[:3000]  # Prevent overflow

    prompt = f"""
<instruction>
You are StudyMate, an AI that answers questions using the retrieved PDF text.
Always cite the page numbers in the format [p:3].
</instruction>

<context>
{retrieved_text}
</context>

<question>
{query}
</question>

<response>
"""

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(
        **inputs,
        max_new_tokens=200,
        temperature=0.2
    )

    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return answer, "Answer generated."


# ---------------------------
# ğŸ“Œ SMART SEARCH (FIXED)
# ---------------------------
def smart_search(query):
    global index, chunks, chunk_page_map

    if index is None:
        return "âš  Upload PDF first!"

    q_emb = embedder.encode([query]).astype("float32")
    D, I = index.search(q_emb, 5)

    results = []
    for idx in I[0]:
        page = chunk_page_map[idx]
        text = chunks[idx]
        results.append(f"[p:{page}] {text}")

    return "\n\n---\n\n".join(results)


# ---------------------------
# ğŸ“Œ SUMMARY GENERATOR (SAFE)
# ---------------------------
def generate_summary():
    full_text = "\n".join(pdf_text_pages)
    full_text = full_text[:3000]     # Avoid overflow

    prompt = f"""
<instruction>
Summarize the following PDF into clear study notes.
</instruction>

<text>
{full_text}
</text>

<response>
"""

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=250)
    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)

    return summary


# ---------------------------
# ğŸ¨ GRADIO UI
# ---------------------------
with gr.Blocks() as app:

    gr.Markdown("# ğŸ“š StudyMate â€“ AI-Powered PDF System (Fixed Version)")

    pdf = gr.File(label="Upload PDF")
    upload_btn = gr.Button("Parse PDF")
    parse_output = gr.Textbox()

    upload_btn.click(parse_pdf, inputs=pdf, outputs=parse_output)

    gr.Markdown("## ğŸ” Ask Questions")
    question = gr.Textbox()
    answer = gr.Textbox()
    status = gr.Textbox()

    ask_btn = gr.Button("Answer")
    ask_btn.click(ask_question, inputs=question, outputs=[answer, status])

    gr.Markdown("## ğŸ” Smart Search")
    search_query = gr.Textbox()
    search_out = gr.Textbox()

    search_btn = gr.Button("Search")
    search_btn.click(smart_search, inputs=search_query, outputs=search_out)

    gr.Markdown("## ğŸ“ Summary")
    summary_out = gr.Textbox()
    summary_btn = gr.Button("Generate Summary")

    summary_btn.click(generate_summary, outputs=summary_out)

app.launch()